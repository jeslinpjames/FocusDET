{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After conv1 in STCF_EANet: torch.Size([1, 32, 640, 640])\n",
      "After conv2 in STCF_EANet: torch.Size([1, 64, 320, 320])\n",
      "After s2d_1 in STCF_EANet: torch.Size([1, 256, 160, 160])\n",
      "After c3_1 in STCF_EANet: torch.Size([1, 128, 160, 160])\n",
      "After conv3 in STCF_EANet: torch.Size([1, 128, 80, 80])\n",
      "After s2d_2 in STCF_EANet: torch.Size([1, 512, 40, 40])\n",
      "After c3_2 in STCF_EANet: torch.Size([1, 256, 40, 40])\n",
      "After conv4 in STCF_EANet: torch.Size([1, 256, 20, 20])\n",
      "After s2d_3 in STCF_EANet: torch.Size([1, 1024, 10, 10])\n",
      "After c3_3 in STCF_EANet: torch.Size([1, 512, 10, 10])\n",
      "After conv5 in STCF_EANet: torch.Size([1, 512, 10, 10])\n",
      "After s2d_4 in STCF_EANet: torch.Size([1, 2048, 5, 5])\n",
      "After conv1 in AttentionLePEC3: torch.Size([1, 2048, 5, 5])\n",
      "After c3 in AttentionLePEC3: torch.Size([1, 2048, 5, 5])\n",
      "After attention_lepe in AttentionLePEC3: torch.Size([1, 2048, 5, 5])\n",
      "After conv2 in AttentionLePEC3: torch.Size([1, 2048, 5, 5])\n",
      "After attention_lepec3 in STCF_EANet: torch.Size([1, 2048, 5, 5])\n",
      "After sppf in STCF_EANet: torch.Size([1, 512, 5, 5])\n",
      "After backbone in FocusDet: torch.Size([1, 512, 5, 5])\n",
      "After Conv: torch.Size([1, 256, 5, 5])\n",
      "After Upsample: torch.Size([1, 256, 10, 10])\n",
      "After c3_1 in Neck: torch.Size([1, 256, 10, 10])\n",
      "After Conv: torch.Size([1, 128, 10, 10])\n",
      "After Upsample: torch.Size([1, 128, 40, 40])\n",
      "After c3_2 in Neck: torch.Size([1, 128, 40, 40])\n",
      "After Conv: torch.Size([1, 64, 40, 40])\n",
      "After Upsample: torch.Size([1, 64, 160, 160])\n",
      "After c3_3 in Neck: torch.Size([1, 64, 160, 160])\n",
      "After Conv: torch.Size([1, 32, 160, 160])\n",
      "After Upsample: torch.Size([1, 128, 40, 40])\n",
      "After Upsample: torch.Size([1, 128, 160, 160])\n",
      "After c3_4 in Neck: torch.Size([1, 32, 160, 160])\n",
      "After Conv: torch.Size([1, 32, 160, 160])\n",
      "After Upsample: torch.Size([1, 64, 160, 160])\n",
      "After c3_5 in Neck: torch.Size([1, 32, 160, 160])\n",
      "After neck in FocusDet: torch.Size([1, 32, 160, 160])\n",
      "After conv1 in Head: torch.Size([1, 32, 160, 160])\n",
      "After conv2 in Head: torch.Size([1, 18, 160, 160])\n",
      "After view and permute in Head: torch.Size([1, 3, 160, 160, 6])\n",
      "After head in FocusDet: torch.Size([1, 3, 160, 160, 6])\n",
      "Final output shape: torch.Size([1, 3, 160, 160, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SpaceToDepth(nn.Module):\n",
    "    def __init__(self, block_size=2):\n",
    "        super(SpaceToDepth, self).__init__()\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size()\n",
    "        assert H % self.block_size == 0 and W % self.block_size == 0\n",
    "        new_C = C * (self.block_size ** 2)\n",
    "        new_H = H // self.block_size\n",
    "        new_W = W // self.block_size\n",
    "        x = x.view(N, C, new_H, self.block_size, new_W, self.block_size)\n",
    "        x = x.permute(0, 1, 3, 5, 2, 4).contiguous()\n",
    "        x = x.view(N, new_C, new_H, new_W)\n",
    "        return x\n",
    "\n",
    "# Define the Bottleneck module\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, shortcut=True):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.shortcut = shortcut\n",
    "        if self.shortcut:\n",
    "            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.shortcut:\n",
    "            out += self.conv3(identity)\n",
    "        return out\n",
    "\n",
    "# Define the C3 module\n",
    "class C3(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_bottlenecks=3):\n",
    "        super(C3, self).__init__()\n",
    "        self.bottlenecks = nn.ModuleList()\n",
    "        self.bottlenecks.append(Bottleneck(in_channels, out_channels))\n",
    "        for _ in range(1, num_bottlenecks):\n",
    "            self.bottlenecks.append(Bottleneck(out_channels, out_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for bottleneck in self.bottlenecks:\n",
    "            x = bottleneck(x)\n",
    "        return x\n",
    "\n",
    "# Define the AttentionLePE module\n",
    "class AttentionLePE(nn.Module):\n",
    "    def __init__(self, channels, num_heads=4):\n",
    "        super(AttentionLePE, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, kernel_size=1, bias=False)\n",
    "        self.attention = nn.MultiheadAttention(channels, num_heads)\n",
    "        self.proj = nn.Conv2d(channels, channels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        qkv = self.qkv(x).reshape(N, 3, self.num_heads, C // self.num_heads, H, W)\n",
    "        q, k, v = qkv.unbind(dim=1)\n",
    "        q = q.flatten(3).permute(2, 0, 1, 3).reshape(-1, N, C)\n",
    "        k = k.flatten(3).permute(2, 0, 1, 3).reshape(-1, N, C)\n",
    "        v = v.flatten(3).permute(2, 0, 1, 3).reshape(-1, N, C)\n",
    "        out, _ = self.attention(q, k, v)\n",
    "        out = out.reshape(H, W, N, C).permute(2, 3, 0, 1).contiguous()\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "# Define the complete AttentionLePEC3 module\n",
    "class AttentionLePEC3(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_bottlenecks=3, num_heads=4):\n",
    "        super(AttentionLePEC3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.c3 = C3(out_channels, out_channels, num_bottlenecks)\n",
    "        self.attention_lepe = AttentionLePE(out_channels, num_heads)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        print(f\"After conv1 in AttentionLePEC3: {x.shape}\")\n",
    "        x = self.c3(x)\n",
    "        print(f\"After c3 in AttentionLePEC3: {x.shape}\")\n",
    "        attention_output = self.attention_lepe(x)\n",
    "        print(f\"After attention_lepe in AttentionLePEC3: {attention_output.shape}\")\n",
    "        x = x + attention_output  # Apply skip connection\n",
    "        x = self.conv2(x)\n",
    "        print(f\"After conv2 in AttentionLePEC3: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "class SPPF(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SPPF, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=5, stride=1, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=9, stride=1, padding=4)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=13, stride=1, padding=6)\n",
    "        self.conv2 = nn.Conv2d(out_channels * 4, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        p1 = self.pool1(x)\n",
    "        p2 = self.pool2(x)\n",
    "        p3 = self.pool3(x)\n",
    "        out = torch.cat([x, p1, p2, p3], dim=1)\n",
    "        out = self.conv2(out)\n",
    "        return out\n",
    "\n",
    "class STCF_EANet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(STCF_EANet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.s2d_1 = SpaceToDepth(block_size=2)\n",
    "        self.c3_1 = C3(256, 128)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.s2d_2 = SpaceToDepth(block_size=2)\n",
    "        self.c3_2 = C3(512, 256)\n",
    "        self.conv4 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.s2d_3 = SpaceToDepth(block_size=2)\n",
    "        self.c3_3 = C3(1024, 512)\n",
    "        self.conv5 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.s2d_4 = SpaceToDepth(block_size=2)\n",
    "        self.attention_lepec3 = AttentionLePEC3(2048, 2048, num_bottlenecks=3, num_heads=4)  # Added AttentionLePEC3 module\n",
    "        self.sppf = SPPF(2048, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        print(f\"After conv1 in STCF_EANet: {x.shape}\")\n",
    "        x = self.conv2(x)\n",
    "        print(f\"After conv2 in STCF_EANet: {x.shape}\")\n",
    "        x = self.s2d_1(x)\n",
    "        print(f\"After s2d_1 in STCF_EANet: {x.shape}\")\n",
    "        x = self.c3_1(x)\n",
    "        print(f\"After c3_1 in STCF_EANet: {x.shape}\")\n",
    "        skip1 = x\n",
    "        x = self.conv3(x)\n",
    "        print(f\"After conv3 in STCF_EANet: {x.shape}\")\n",
    "        x = self.s2d_2(x)\n",
    "        print(f\"After s2d_2 in STCF_EANet: {x.shape}\")\n",
    "        x = self.c3_2(x)\n",
    "        print(f\"After c3_2 in STCF_EANet: {x.shape}\")\n",
    "        skip2 = x\n",
    "        x = self.conv4(x)\n",
    "        print(f\"After conv4 in STCF_EANet: {x.shape}\")\n",
    "        x = self.s2d_3(x)\n",
    "        print(f\"After s2d_3 in STCF_EANet: {x.shape}\")\n",
    "        x = self.c3_3(x)\n",
    "        print(f\"After c3_3 in STCF_EANet: {x.shape}\")\n",
    "        skip3 = x\n",
    "        x = self.conv5(x)\n",
    "        print(f\"After conv5 in STCF_EANet: {x.shape}\")\n",
    "        x = self.s2d_4(x)\n",
    "        print(f\"After s2d_4 in STCF_EANet: {x.shape}\")\n",
    "        x = self.attention_lepec3(x)  # Use the AttentionLePEC3 module\n",
    "        print(f\"After attention_lepec3 in STCF_EANet: {x.shape}\")\n",
    "        x = self.sppf(x)\n",
    "        print(f\"After sppf in STCF_EANet: {x.shape}\")\n",
    "        return x, [skip3, skip2, skip1]\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(Conv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        print(f\"After Conv: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, scale_factor=2, mode='nearest'):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=scale_factor, mode=mode)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        print(f\"After Upsample: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "class Neck(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Neck, self).__init__()\n",
    "        self.conv1 = Conv(512, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.upsample1 = Upsample()\n",
    "        self.c3_1 = C3(768, 256)  # First C3 block after concatenation (512 from upsampled + 256 from skip1)\n",
    "\n",
    "        self.conv2 = Conv(256, 128, kernel_size=1, stride=1, padding=0)\n",
    "        self.upsample2 = Upsample(scale_factor=4)\n",
    "        self.c3_2 = C3(384, 128)  # Second C3 block after concatenation (256 from upsampled + 128 from skip2)\n",
    "\n",
    "        self.conv3 = Conv(128, 64, kernel_size=1, stride=1, padding=0)\n",
    "        self.upsample3 = Upsample(scale_factor=4)\n",
    "        self.c3_3 = C3(192, 64)  # Third C3 block after concatenation (128 from upsampled + 64 from skip3)\n",
    "\n",
    "        self.conv4 = Conv(64, 32, kernel_size=1, stride=1, padding=0)\n",
    "        self.c3_4 = C3(160, 32)  # Fourth C3 block after concatenation (64 from conv4 + 32 from skip4)\n",
    "\n",
    "        self.conv5 = Conv(32, 32, kernel_size=1, stride=1, padding=0)\n",
    "        self.c3_5 = C3(96, 32)  # Fifth C3 block after concatenation (32 from conv5 + 32 from skip5)\n",
    "\n",
    "    def forward(self, x, skip_connections):\n",
    "        x = self.conv1(x)\n",
    "        x = self.upsample1(x)\n",
    "        x = torch.cat([x, skip_connections[0]], dim=1)  # Concatenation with skip connection 1 (from c3_3 in STCF_EANet)\n",
    "        x = self.c3_1(x)\n",
    "        print(f\"After c3_1 in Neck: {x.shape}\")\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        skip4 = x\n",
    "        x = self.upsample2(x)\n",
    "        x = torch.cat([x, skip_connections[1]], dim=1)  # Concatenation with skip connection 2 (from c3_2 in STCF_EANet)\n",
    "        x = self.c3_2(x)\n",
    "        print(f\"After c3_2 in Neck: {x.shape}\")\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        skip5 = x\n",
    "        x = self.upsample3(x)\n",
    "        x = torch.cat([x, skip_connections[2]], dim=1)  # Concatenation with skip connection 3 (from c3_1 in STCF_EANet)\n",
    "        x = self.c3_3(x)\n",
    "        print(f\"After c3_3 in Neck: {x.shape}\")\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        skip4 = self.upsample2(skip4)\n",
    "        skip4 = self.upsample3(skip4)\n",
    "        x = torch.cat([x, skip4], dim=1)  # Concatenation with skip connection 4 (from conv2 in Neck)\n",
    "        x = self.c3_4(x)\n",
    "        print(f\"After c3_4 in Neck: {x.shape}\")\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        skip5 = self.upsample3(skip5)\n",
    "        x = torch.cat([x, skip5], dim=1)  # Concatenation with skip connection 5 (from conv3 in Neck)\n",
    "        x = self.c3_5(x)\n",
    "        print(f\"After c3_5 in Neck: {x.shape}\")\n",
    "\n",
    "        return x\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, num_anchors):\n",
    "        super(Head, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels, num_anchors * (num_classes + 5), kernel_size=1, stride=1, padding=0)\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        print(f\"After conv1 in Head: {x.shape}\")\n",
    "        x = self.conv2(x)\n",
    "        print(f\"After conv2 in Head: {x.shape}\")\n",
    "        batch_size, _, height, width = x.shape\n",
    "        x = x.view(batch_size, self.num_anchors, self.num_classes + 5, height, width)\n",
    "        x = x.permute(0, 1, 3, 4, 2).contiguous()\n",
    "        print(f\"After view and permute in Head: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "class FocusDet(nn.Module):\n",
    "    def __init__(self, num_classes, num_anchors):\n",
    "        super(FocusDet, self).__init__()\n",
    "        self.backbone = STCF_EANet()\n",
    "        self.neck = Neck()\n",
    "        self.head = Head(32, num_classes, num_anchors)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the backbone and collect skip connections\n",
    "        x, skip_connections = self.backbone(x)\n",
    "        print(f\"After backbone in FocusDet: {x.shape}\")\n",
    "\n",
    "        # Pass through the neck\n",
    "        neck_output = self.neck(x, skip_connections)\n",
    "        print(f\"After neck in FocusDet: {neck_output.shape}\")\n",
    "\n",
    "        # Pass through the head\n",
    "        head_output = self.head(neck_output)\n",
    "        print(f\"After head in FocusDet: {head_output.shape}\")\n",
    "\n",
    "        return head_output\n",
    "\n",
    "# Example to test the FocusDet module\n",
    "model = FocusDet(num_classes=1, num_anchors=3)\n",
    "dummy_input = torch.randn(1, 3, 640, 640)\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(\"Final output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Output shape: torch.Size([1, 3, 160, 160, 6])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Example to test the model\n",
    "model = FocusDet(num_classes=1, num_anchors=3).to(device)\n",
    "dummy_input = torch.randn(1, 3, 640, 640).to(device)  # Batch size 1, 3 RGB channels, 640x640 image\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FocusDet(num_classes=1, num_anchors=3).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jesli\\anaconda3\\envs\\pv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Total number of images in train dataset: 19204\n",
      "Total number of images in val dataset: 865\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 212/1921 [05:02<40:38,  1.43s/it, loss=1.14] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 173\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 173\u001b[0m     train_losses \u001b[38;5;241m=\u001b[39m train_fn(train_loader, model, optimizer, \u001b[38;5;28;01mlambda\u001b[39;00m preds, targets: yolo_loss(preds, targets, bbox_scale, obj_scale, class_scale), device)\n\u001b[0;32m    174\u001b[0m     train_loss_history\u001b[38;5;241m.\u001b[39mextend(train_losses)\n\u001b[0;32m    176\u001b[0m     val_loss, val_losses \u001b[38;5;241m=\u001b[39m eval_fn(val_loader, model, \u001b[38;5;28;01mlambda\u001b[39;00m preds, targets: yolo_loss(preds, targets, bbox_scale, obj_scale, class_scale), device)\n",
      "Cell \u001b[1;32mIn[5], line 140\u001b[0m, in \u001b[0;36mtrain_fn\u001b[1;34m(train_loader, model, optimizer, loss_fn, device)\u001b[0m\n\u001b[0;32m    137\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# Update progress bar\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m     loop\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    141\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_losses\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Dataset and DataLoader\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, images_folder, labels_folder, image_size=640, transform=None):\n",
    "        self.images_folder = images_folder\n",
    "        self.labels_folder = labels_folder\n",
    "        self.image_size = image_size\n",
    "        self.image_files = [f for f in os.listdir(images_folder) if os.path.isfile(os.path.join(images_folder, f))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(f\"Loading index: {idx}\")\n",
    "        image_path = os.path.join(self.images_folder, self.image_files[idx])\n",
    "        label_path = os.path.join(self.labels_folder, os.path.splitext(self.image_files[idx])[0] + \".txt\")\n",
    "        # print(f\"Image path: {image_path}, Label path: {label_path}\")\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # print(f\"Loaded image shape: {image.shape}\")\n",
    "\n",
    "        # Load labels\n",
    "        boxes = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        class_id, x_center, y_center, width, height = map(float, line.split())\n",
    "                        boxes.append([class_id, x_center, y_center, width, height])\n",
    "\n",
    "        # If no labels, add a dummy box with zeros\n",
    "        if len(boxes) == 0:\n",
    "            boxes.append([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "        boxes = torch.tensor(boxes)\n",
    "        # print(f\"Loaded boxes for index {idx}, shape: {boxes.shape}\")\n",
    "        return image, boxes\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = YOLODataset(images_folder=\"data_full/train/images\", labels_folder=\"data_full/train/labels\", transform=transform)\n",
    "val_dataset = YOLODataset(images_folder=\"data_full/val/images\", labels_folder=\"data_full/val/labels\", transform=transform)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = list(zip(*batch))\n",
    "    images = torch.stack(images, dim=0)\n",
    "    max_len = max(len(t) for t in targets)\n",
    "    padded_targets = []\n",
    "    for t in targets:\n",
    "        if t.shape[0] == 0:\n",
    "            padded_t = torch.zeros((1, 5))\n",
    "        else:\n",
    "            padded_t = torch.zeros((max_len, 5))\n",
    "            padded_t[:len(t), :] = t\n",
    "        padded_targets.append(padded_t)\n",
    "    targets = torch.stack(padded_targets, dim=0)\n",
    "    return images, targets\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# Print the total number of images in train and val datasets\n",
    "print(f\"Total number of images in train dataset: {len(train_dataset)}\")\n",
    "print(f\"Total number of images in val dataset: {len(val_dataset)}\")\n",
    "\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "def yolo_loss(preds, targets, bbox_scale=1.0, obj_scale=1.0, class_scale=1.0):\n",
    "    batch_size, _, grid_size, _, _ = preds.shape\n",
    "\n",
    "    # Create new target tensors with the same shape as predictions\n",
    "    new_target_boxes = torch.zeros_like(preds[..., 1:5]).to(device)\n",
    "    new_target_obj = torch.zeros_like(preds[..., 0]).to(device)\n",
    "    new_target_class = torch.zeros_like(preds[..., 5:]).to(device)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for box in targets[i]:\n",
    "            if box.sum() == 0:  # Skip padding and handle no objects case\n",
    "                continue\n",
    "\n",
    "            # Calculate which grid cell this ground truth box belongs to\n",
    "            grid_x = int(box[1] * grid_size)\n",
    "            grid_y = int(box[2] * grid_size)\n",
    "\n",
    "            # Update the corresponding elements in the new target tensors\n",
    "            new_target_boxes[i, :, grid_y, grid_x] = box[1:5]\n",
    "            new_target_obj[i, :, grid_y, grid_x] = 1\n",
    "            new_target_class[i, :, grid_y, grid_x] = box[0]\n",
    "\n",
    "    # Now calculate losses using the reshaped targets\n",
    "    bbox_loss = F.mse_loss(preds[..., 1:5], new_target_boxes)\n",
    "    obj_loss = F.mse_loss(preds[..., 0], new_target_obj)\n",
    "    class_loss = F.cross_entropy(preds[..., 5:].view(-1, preds.size(-1)-5), new_target_class.long().view(-1))\n",
    "\n",
    "    total_loss = bbox_scale * bbox_loss + obj_scale * obj_loss + class_scale * class_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def train_fn(train_loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    train_losses = []\n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(data)\n",
    "        loss = loss_fn(predictions, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        train_losses.append(loss.item())\n",
    "    return train_losses\n",
    "\n",
    "def eval_fn(val_loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, targets) in enumerate(val_loader):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    print(f\"Validation Loss: {mean_loss:.4f}\")\n",
    "    return mean_loss, losses\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "best_loss = float(\"inf\")\n",
    "bbox_scale = 4000.0\n",
    "obj_scale = 4000.0\n",
    "class_scale = 4000.0\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    train_losses = train_fn(train_loader, model, optimizer, lambda preds, targets: yolo_loss(preds, targets, bbox_scale, obj_scale, class_scale), device)\n",
    "    train_loss_history.extend(train_losses)\n",
    "    \n",
    "    val_loss, val_losses = eval_fn(val_loader, model, lambda preds, targets: yolo_loss(preds, targets, bbox_scale, obj_scale, class_scale), device)\n",
    "    val_loss_history.extend(val_losses)\n",
    "\n",
    "    # Save model checkpoint\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), \"final_model.pth\")\n",
    "\n",
    "# Plot the training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_history, label='Train Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Batches')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated image saved to annotated_IP1.jpg\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "num_classes = 1\n",
    "num_anchors = 3\n",
    "model = FocusDet(num_classes, num_anchors).to(device)\n",
    "model.load_state_dict(torch.load(\"final_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Define the transform to preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def annotate_image(image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_transformed = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        preds = model(image_transformed)\n",
    "\n",
    "    # Process predictions\n",
    "    batch_size, num_anchors, grid_size, _, _ = preds.shape\n",
    "    preds = preds.cpu().numpy()\n",
    "    \n",
    "    boxes = []\n",
    "    for i in range(num_anchors):\n",
    "        for y in range(grid_size):\n",
    "            for x in range(grid_size):\n",
    "                pred = preds[0, i, y, x, :]\n",
    "                obj_score = pred[0]\n",
    "                if obj_score > 0.5:  # Confidence threshold\n",
    "                    class_id = int(pred[5:].argmax())\n",
    "                    x_center, y_center, width, height = pred[1:5]\n",
    "                    x_center = x_center * image.width / grid_size\n",
    "                    y_center = y_center * image.height / grid_size\n",
    "                    width = width * image.width / grid_size\n",
    "                    height = height * image.height / grid_size\n",
    "                    x1 = int(x_center - width / 2)\n",
    "                    y1 = int(y_center - height / 2)\n",
    "                    x2 = int(x_center + width / 2)\n",
    "                    y2 = int(y_center + height / 2)\n",
    "                    boxes.append((x1, y1, x2, y2, obj_score, class_id))\n",
    "\n",
    "    # Draw boxes on the image\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2, score, class_id = box\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "        draw.text((x1, y1), f\"{class_id}: {score:.2f}\", fill=\"red\")\n",
    "\n",
    "    # Save or display the annotated image\n",
    "    annotated_image_path = \"annotated_\" + os.path.basename(image_path)\n",
    "    image.save(annotated_image_path)\n",
    "    print(f\"Annotated image saved to {annotated_image_path}\")\n",
    "\n",
    "# Example usage\n",
    "annotate_image(\"data/train/images/IP1.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STCF_EANet Conv1: torch.Size([1, 32, 640, 640])\n",
      "STCF_EANet Conv2: torch.Size([1, 64, 320, 320])\n",
      "STCF_EANet SpaceToDepth1: torch.Size([1, 256, 160, 160])\n",
      "STCF_EANet C3_1: torch.Size([1, 128, 160, 160])\n",
      "STCF_EANet Conv3: torch.Size([1, 128, 80, 80])\n",
      "STCF_EANet SpaceToDepth2: torch.Size([1, 512, 40, 40])\n",
      "STCF_EANet C3_2: torch.Size([1, 256, 40, 40])\n",
      "STCF_EANet Conv4: torch.Size([1, 256, 20, 20])\n",
      "STCF_EANet SpaceToDepth3: torch.Size([1, 1024, 10, 10])\n",
      "STCF_EANet C3_3: torch.Size([1, 512, 10, 10])\n",
      "STCF_EANet Conv5: torch.Size([1, 512, 10, 10])\n",
      "STCF_EANet SpaceToDepth4: torch.Size([1, 2048, 5, 5])\n",
      "STCF_EANet AttentionLePE: torch.Size([1, 2048, 5, 5])\n",
      "SPPF Conv1: torch.Size([1, 512, 5, 5])\n",
      "SPPF Output: torch.Size([1, 512, 5, 5])\n",
      "STCF_EANet SPPF: torch.Size([1, 512, 5, 5])\n",
      "Conv: torch.Size([1, 256, 5, 5])\n",
      "Upsample: torch.Size([1, 256, 10, 10])\n",
      "Neck Upsample1: torch.Size([1, 256, 10, 10])\n",
      "Neck C3_1: torch.Size([1, 256, 10, 10])\n",
      "Conv: torch.Size([1, 128, 10, 10])\n",
      "Upsample: torch.Size([1, 128, 40, 40])\n",
      "Neck Upsample2: torch.Size([1, 128, 40, 40])\n",
      "Neck C3_2: torch.Size([1, 128, 40, 40])\n",
      "Conv: torch.Size([1, 64, 40, 40])\n",
      "Upsample: torch.Size([1, 64, 160, 160])\n",
      "Neck Upsample3: torch.Size([1, 64, 160, 160])\n",
      "Neck C3_3: torch.Size([1, 64, 160, 160])\n",
      "Conv: torch.Size([1, 32, 160, 160])\n",
      "Upsample: torch.Size([1, 128, 40, 40])\n",
      "Upsample: torch.Size([1, 128, 160, 160])\n",
      "Neck C3_4: torch.Size([1, 32, 160, 160])\n",
      "Conv: torch.Size([1, 32, 160, 160])\n",
      "Upsample: torch.Size([1, 64, 160, 160])\n",
      "Neck C3_5: torch.Size([1, 32, 160, 160])\n",
      "Head Conv1: torch.Size([1, 32, 160, 160])\n",
      "Head Conv2: torch.Size([1, 18, 160, 160])\n",
      "Head Output: torch.Size([1, 3, 160, 160, 6])\n",
      "Output shape: torch.Size([1, 3, 160, 160, 6])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Load and transform the image\n",
    "image_path = \"data/train/images/IP17.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image = transform(image)\n",
    "image = image.unsqueeze(0).to(device)  # Add batch dimension and move to the appropriate device\n",
    "\n",
    "# Pass the transformed image through the model\n",
    "output = model(image)\n",
    "\n",
    "# Display the output dimension\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
